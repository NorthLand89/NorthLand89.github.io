<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  AI迷思
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="AI迷思" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:www.aimyth.com ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="point.html">心得</a></li>
        
        <li id=""><a target="_self" href="algorithm.html">算法</a></li>
        
        <li id=""><a target="_self" href="tech.html">技术栈</a></li>
        
        <li id=""><a target="_self" href="doc.html">文档</a></li>
        
        <li id=""><a target="_self" href="course.html">课程</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="note.html">课程笔记</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; AI迷思</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="note.html">课程笔记</a></li>
        
            <li><a href="point.html">心得</a></li>
        
            <li><a href="course.html">课程</a></li>
        
            <li><a href="doc.html">文档</a></li>
        
            <li><a href="algorithm.html">算法</a></li>
        
            <li><a href="tech.html">技术栈</a></li>
        
            <li><a href="data.html">data</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="15093533296418.html">
                
                  <h1>软件工程笔记</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">1.软件过程</h2>

<p><code>软件过程</code>也称为<code>软件生存周期过程</code>，是指软件生存周期中的一系列相关过程。其中<code>过程</code>就是<code>活动</code>的集合，活动是<code>任务</code>的集合，<strong>任务把输入加工成输出</strong>。活动的执行可以是顺序的、迭代的、并行的、嵌套的、有条件引发的。</p>

<h3 id="toc_1">经典软件过程模型的特点</h3>

<h4 id="toc_2">瀑布模型</h4>

<p>将基本的开发活动看成一系列界限分明的独立阶段，是一种计划驱动的软件过程，有利于规范软件开发活动。<br/>
优点：</p>

<ol>
<li>简洁</li>
<li>在支持开发结构化软件，控制软件开发复杂度，促进软件开发工程化中起了显著作用。</li>
<li>通过开发计划的制定、项目估算、阶段评审和文档控制有效地对软件过程进行指导，从而对软件质量有一定程度的保证。</li>
</ol>

<p>缺点：</p>

<ol>
<li>无法接受需求的不确定性。</li>
<li>错误发现容易太迟，导致返工。</li>
<li>作为线性模型，无开发者常被不必要地单个，一些项目成员要等其他成员先完成依赖任务后开始开发。</li>
</ol>

<h4 id="toc_3">增量模型</h4>

<p>增量式开发的思想是先开发出一个初始的实现，给用户使用并听取用户的使用意见和建议，通过多个版本的不断修改直到产生一个充分的系统。</p>

<p>描述，开发和验证交织在一起。</p>

<p>优点：</p>

<ol>
<li>降低了适用用户需求变更的成本。</li>
<li>在开发过程中更容易得到用户对于已做的开发工作的反馈意见。</li>
<li>使更快地交付和部署有用的软件到客户方变成可能。</li>
</ol>

<p>缺点：<br/>
1. 过程不可见。<br/>
2. 伴随着新增量的添加，系统结构在不断退化。随着时间推移，变更成本逐渐上升。</p>

<p>与瀑布模型相比的特点：<br/>
1. 多个版本可以并行开发。<br/>
2. 每个版本都是可运行版本。<br/>
3. 需求在开发早起是明确的。</p>

<h4 id="toc_4">演化模型</h4>

<p>演化模型是迭代的过程模型，支持并行开发。</p>

<ol>
<li>演化模型在一部分需求被定义后就开始开发了，在每个相继的版本中逐步完善。</li>
<li>允许需求变更，通过演示迭代产生部分系统功能，尽早手机用户对系统的反馈，及时改正对用户需求的理解偏差。</li>
</ol>

<p>常见的是原型模型和螺旋模型。</p>

<h4 id="toc_5">统一过程模型</h4>

<p>是一种风险驱动的软件过程模型。</p>

<p>它基于UML和构件式架构的迭代，演化开发过程</p>

<p>将软件开发生命周期分为：<br/>
* 先启-目标里程碑<br/>
* 精化-架构里程碑<br/>
* 构建-初始能力里程碑<br/>
* 产品化-发布里程碑</p>

<p>四个阶段。</p>

<p>每个阶段可以细分成若干个迭代。</p>

<p>是最被广泛接受的模型。</p>

<h4 id="toc_6">演化模型与增量模型区别</h4>

<p>增量模型在需求设计阶段是整体进行的，在编码测试阶段是渐增进行的。演化模型全部系统是增量开发，增量提交。</p>

<h3 id="toc_7">过程评估与CMM/CMMI</h3>

<p><code>过程评估</code><br/>
定义的软件过程是否适合本组织或本团队？<br/>
过程绩效如何？<br/>
软件过程能力成熟度为几级？<br/>
哪些过程域做的很好，哪些又有不足？</p>

<p><code>CMM</code>：软件成熟度模型。<br/>
<code>CMMI</code>CMM威力加强版，软件能力成熟度模型集成。</p>

<p>CMMI包括：<br/>
* CMMI for Development：开发模型，提供软件开发过程的管理，测量，监控指南。<br/>
* CMMI for Service：服务模型，提供组织内部向外部客户的服务交付指南。<br/>
* CMMI for Acquisition：采购模型，提供软件采购和外包管理的服务交付指南。</p>

<p>CMMI模型用不同的表示法支持不同的改进方法：</p>

<ul>
<li><p><code>组织成熟度方法</code>：阶梯式模型，强调组织成熟度，从过程域集合的角度考察整个组织的过程成熟度阶段。</p></li>
<li><p><code>过程能力方法</code>：连续式模型，强调单个过程域的能力，从过程域的角度考察基线和度量结果的改善。</p></li>
</ul>

<p>阶梯式模型将软件组织的成熟度分为以下5级：</p>

<ol>
<li>初始级：无序混乱的。</li>
<li>已管理级：有管理过程了。</li>
<li>已定义级：软件管理和工程两方面标准化了，就像现在公司一样。</li>
<li>定量管理级：软件过程和产品质量 有详细的度量一举，对软件过程和产品有定量的理解和控制了。</li>
<li>持续优化级：过程的量化反馈和先进的新思想，新技术促使过程持续不断改进。</li>
</ol>

<h3 id="toc_8">敏捷宣言与敏捷过程特点</h3>

<h4 id="toc_9">4条敏捷价值观</h4>

<ol>
<li>注重个人交互胜于过程和工具</li>
<li>注重可用的软件胜于事无巨细的文档</li>
<li>注重客户协作胜于合同谈判</li>
<li>注重随机应变胜于恪守计划</li>
</ol>

<h4 id="toc_10">12条基本规则</h4>

<ol>
<li>最优先的目标是通过尽早地，持续低交付高价值的软件来满足客户需求。</li>
<li>欢迎需求变化，甚至在开发后期。</li>
<li>经常交付可用软件，间隔从两周到两个月不等，优先采用较短的时间尺度。</li>
<li>整个项目自始至终，业务人员和开发人员都必须每天在一起工作。</li>
<li>以积极主动的人员为核心建立项目团队，给与他们所需的环境和支持，并且信任他们能胜任工作。</li>
<li>开发团队内外传递信息最有效的方法是面对面的交流。</li>
<li>可用的软件是最主要的项目进展指标。</li>
<li>敏捷过程提倡可持续的开发。项目发起人、开发者、用户都应始终保持稳定的开发步调。</li>
<li>持续关注技术上的精益求精和优良的设计以增强敏捷性。</li>
<li>简约，将必要的工作最小化的艺术，是成功的关键。</li>
<li>最优的架构、需求、设计浮现于自组织的团队。</li>
<li><p>团队定期不断对如何更加有效地开展工作进行反思，并相应地调整，校正自己的行为。</p>

<h4 id="toc_11">适合采用敏捷过程的场景</h4></li>
</ol>

<ul>
<li>需求不确定、易挥发</li>
<li>有责任感和积极向上的开发人员</li>
<li>用户容易沟通并能参与</li>
<li>团队小于10人</li>
<li>
#### 特点
敏捷过程保留了传统软件实践的基本框架活动：客户沟通，策划，建模，构建，测试，交付，迭代等，但将其所见到一个推动项目组朝着构建和交付发展的最小任务集。</li>
</ul>

<h2 id="toc_12">2.软件需求</h2>

<p><code>需求</code>就是系统必须符合的条件或能力。</p>

<p>FURPS+模型：</p>

<p>功能性：F，特性，功能和安全性。<br/>
易用性：U，描述实现“用户友好”的因素，几分钟上手啥的。<br/>
可靠性：R，无故障执行一段时间的概率。<br/>
性能：P,在功能需求基础上规定的性能参数，包括速度，效率，可用性，准确性，吞吐量，响应时间或资源使用情况。<br/>
可支持性：S,系统使用各活动所需工作量的大小。</p>

<p>URPS合成非功能需求</p>

<p>+是一些补充的需求：<br/>
设计约束：比如必须采用某种算法。<br/>
实现需求：比如语言，策略等。<br/>
接口需求：与之交互的外部软件或硬件。<br/>
无力需求：必须具备的物理特征，代表硬件要求。</p>

<p>需求工程的基本过程：<br/>
* 需求获取<br/>
* 需求分析<br/>
* 需求定义<br/>
* 需求验证<br/>
*（全程）需求管理</p>

<p>软件需求包括3个不同的层次：<br/>
1. 项目干系人需求：原始需求，通过调研得到的项目干系人对系统的要求。<br/>
2. 前景文档：既概要需求，用于记录关键的用户需要和系统特征。<br/>
3. 软件需求规约：既正式的详细软件需求，用于记录详细的功能需求，非功能需求，约束条件。其中功能需求常用用例图来刻画。</p>

<h3 id="toc_13">用例图及其UML表达</h3>

<h4 id="toc_14">用例图</h4>

<p>用例图是一种描述待加建软件上下文范围及它提供的功能的概览视图。从黑盒的角度描述谁与系统交互，外部世界希望系统做些什么。</p>

<p>用例图包含三种要素：</p>

<ul>
<li><p>执行者：Actor，与系统交互的实体。</p></li>
<li><p>用例：执行者希望系统为他们做什么。</p></li>
<li><p>关系：A-C，A-A，C-C 关系。关系分3类 包含、扩展、泛化。</p></li>
</ul>

<p><img src="media/15087884854897/15087888659410.jpg" alt=""/></p>

<h4 id="toc_15">活动图</h4>

<p>活动图用于刻画一个系统或者子系统的工作流程，也可用于描述用例内部的事件。它提供了活动流程的可视化描述，关注被执行的活动及谁负责执行这些活动。</p>

<p>活动图的基本元素包括：<br/>
* 动作，Action，一个活动可以包含多个动作。用圆角矩形表示。<br/>
* 控制流，用来表示从一个动作到另一个动作的流的控制。用带箭头的直线表示。</p>

<h5 id="toc_16">控制节点分为</h5>

<ol>
<li><strong>初始节点</strong>：实心圆表示，是活动开始的节点。</li>
<li><p><strong>终止节点</strong> ：<strong>活动终止</strong>：带十字叉的圆。<strong>流终止</strong>：带边框的实心圆。</p></li>
<li><p><strong>判断节点和合并节点</strong><br/>
用菱形表示，判断节点有一个输入流和多个输出流，所有离去流的监护条件应该：</p>

<ul>
<li>覆盖所有可能性（避免流冻结）</li>
<li>不应该重叠（避免二义性）
合并节点接受多个流的情况下，它的离去流指向的动作会被执行多次。</li>
</ul></li>
<li><p><strong>分叉节点和汇合节点</strong></p></li>
</ol>

<p>用来表示并发流程的开始和结束，用水平或者垂直黑线表示。<img src="media/15087893059804/15087909088140.jpg" alt=""/><br/>
5. <strong>对象节点</strong></p>

<p>是动作处理的数据，用举行表示，某些情况下有用，不推荐泛用。</p>

<h4 id="toc_17">分区/泳道图</h4>

<p>分区图=泳道图</p>

<p>分组的目的是说明执行具体活动的责任。<br/>
<img src="media/15087893059804/15087918675671.jpg" alt=""/></p>

<h4 id="toc_18">时序图（顺序图）</h4>

<p>时序图（序列图，顺序图）关注对象间协作的UML图。</p>

<p>它通过描述对象之间发送消息的时间顺序显示多个对象之间的动态合作。</p>

<p>垂直维是<strong>时间</strong>，表示对象间传递晓得的时间顺序。</p>

<p>水平维是<strong>角色</strong>，代表参与交互的对象。</p>

<p><strong>消息</strong> 从发出者指向接受者，由垂直位置表示。分为：</p>

<ul>
<li><strong>同步消息</strong> 带实心箭头的实线。</li>
<li><strong>异步消息</strong> 开放箭头的实线。</li>
<li><strong>返回消息</strong> 带开放箭头的实线？应该是虚线实体箭头。</li>
</ul>

<p>可以用序列片段（Fragment）简化时序图，也可以更清晰地描述时序图中的流程控制结构。它包括：</p>

<ul>
<li><strong>交互使用</strong>：ref。</li>
<li><strong>循环</strong>：loop。</li>
<li><strong>条件</strong>：alt。</li>
<li><strong>并发</strong>：par
<img src="media/15093533296418/15093566011971.jpg" alt=""/></li>
</ul>

<h3 id="toc_19">数据模型建模及其UML表达</h3>

<h4 id="toc_20">类</h4>

<p>类定义了一组有着状态和行为的对象。状态由属性和关联描述。个体行为由操作来描述，方法是操作的实现。<br/>
<img src="media/15087925660642/15087931338746.jpg" alt=""/></p>

<p><em>书上的例图没有方法返回类型</em></p>

<p>UML支持的可见类型包括：</p>

<ul>
<li>+ public </li>
<li># protected </li>
<li>- private </li>
<li>~ package </li>
</ul>

<h4 id="toc_21">继承/泛化</h4>

<p>一组类可以用繁华关系再起内部建立的继承机制分享公用的状态和行为描述。</p>

<p>类之间的泛化关系用带空心三角的实现来表示，多重继承现实存在，但建模时要注意，他会引起冲突和重复继承问题。</p>

<p>继承是一种 is-a关系，直观但也会引起混淆。</p>

<p>继承关系由子类指向父类。</p>

<h5 id="toc_22">Liskov替换准则</h5>

<p>在继承时，子类应该能替换父类。</p>

<h5 id="toc_23">关联</h5>

<p>两个相对独立的类，当一个类的实例与另一个类的特定实例存在固定关系时，这两个类就存在关联关系，关联的两个连接点成为关联端，在关联端可以设置<strong>名字</strong>，<strong>可见性</strong>，<strong>基数</strong>等特征，</p>

<p><strong>基数</strong>是连接两端的数字，表示这一段的类可以有几个实例。<br/>
<img src="media/15087925660642/15087942571153.jpg" alt=""/></p>

<p>上图有问题哈</p>

<p>箭头由车指向人，车知道主人是谁，但人不知道自己有哪些车，双向关联用直线连接就好。</p>

<p>分析阶段不需要在乎关联的方向，应避免多余的关联。</p>

<p>设计阶段关联用来说明关于数据结构的设计和类之间职责的划分，这个时候关联的方向性很重要，而且为了提高对象的存取效率和对特定类信息的定位，也可引入一些必要的多余关联。</p>

<h5 id="toc_24">聚合</h5>

<p>聚合是一种特殊的关联，表示部分与整体的关系，比如车队是车的聚合，用带空菱形的实线表示，空菱形与聚合类连接（集合类）</p>

<h5 id="toc_25">组合</h5>

<p>组合是一种更强形式的聚合，用带有实菱形的实现表示，比如夫妻-妻子。</p>

<p>聚合与组合的区别在于整体和部分的生存周期是否独立。组合关系的部分依赖于整体存在，整体消亡时部分亦消失。</p>

<h5 id="toc_26">依赖</h5>

<p>当一个类的行为或实现影响了另一个类时，这两个类之间就存在依赖关系。</p>

<p>依赖关系表示两个模型元素之间存在的一种语义关系，被依赖着的某些变化会要求或指示依赖者随之发生变化。根据这个定义，关联和泛化都是依赖关系，但是他们有更特别的语义，所以有自己的名字和详细的语义。</p>

<p><strong>衍型</strong>：在已定义的模型元素的基础上构造一种新的模型元素的方法。衍型的信息内容和形式与已存在的基本模型元素相同，但是含义和用法有所扩展。例如<strong>对类进行扩展，定义一种特殊的类Interface</strong></p>

<p>依赖的拓展（衍型）包括：</p>

<ul>
<li>访问：一个包访问另一个包的内容</li>
<li>绑定：为模板参数指定值，已生成一个新的模型元素</li>
<li>调用：一个类的方法调用其他类的一个操作</li>
<li>创建：一个类创建另一个类的实例</li>
<li>派生：一个实例从另一个实例导出</li>
<li>实例化：一个类的方法创建另一个类的实例</li>
<li>许可：许可一个元素使用另一个元素的内容</li>
<li>实现：规约及其实现之间的映射</li>
<li>精华：两个不同语义层次上的元素之间的映射</li>
<li>发送：信号发射者和接受者之间的关系</li>
<li>替代：一个类支持另一个类的接口和契约，则这个类就可以替代另一个类</li>
<li>跟踪依赖：不同模型中的元素之间存在的链接，但不如映射精确</li>
<li>使用：一个元素为了实现其功能，需要用到已存在的另一个元素，包括调用，实例化，创新，发送等。</li>
</ul>

<h5 id="toc_27">接口和实现</h5>

<p>接口和实现分离使软件模块更加独立，从而增加可复用性，减少维护成本。</p>

<p><strong>接口是一组操作的集合，它是对类、构建、子系统等行为的抽象</strong><br/>
他是一种特殊的类，包含操作但不包含属性，可以用衍型《》或圆圈表示：</p>

<p><img src="media/15087925660642/15087964873602.jpg" alt=""/></p>

<p><strong>实现（Realize）关系是一种特殊的依赖关系，他表示两个模型元素之间的关系：一个模型元素定义了规约（Spec），二另一个模型元素则实现了这个规约</strong><br/>
实现关系用闭合空箭头虚线表示。</p>

<h3 id="toc_28">行为模型建模及其UML表达</h3>

<h4 id="toc_29">状态建模</h4>

<p><code>状态</code>：一个对象生命期内的一个阶段，该阶段中对象要满足一些特定的条件、执行特定的活动、或等待某个（些）事件的发生。</p>

<p><code>事件</code>：可以触发对象状态改变的外部刺激，也就是消息的发出与接收。他决定状态迁移何时发生。</p>

<p><code>状态迁移</code>：是状态之间的关系，当发生一个时间，条件满足就会发生从原状态到目标状态的改变。</p>

<p>UML中的状态（State）表示在某个时间段内：<br/>
* 某个陈述是正确的，比如x-y&gt;0<br/>
* 某个动作正在执行（检查存货）或在某个时间等待触发（等待到货）<br/>
* <br/>
状态图跟踪一个类在系统中全生命周期的状态切换。<br/>
<img src="media/15093533296418/15093730145377.jpg" alt=""/><br/>
有1个起始状态，0个或者多个结束状态。</p>

<p><img src="media/15093533296418/15093737198197.jpg" alt=""/><br/>
<img src="media/15093533296418/15093738384195.jpg" alt=""/><br/>
一个状态只能发生一个迁移，迁移条件一定互斥。事件必须是瞬间完成的动作。<br/>
<img src="media/15093533296418/15093739094494.jpg" alt=""/><br/>
UML中有四种事件：<br/>
* 变更事件：当给定条件成立时就会发生变更事件。是一个条件表达式。<br/>
<img src="media/15093533296418/15093741590166.jpg" alt=""/></p>

<ul>
<li><p>调用事件：当给定对象的操作被调用执行时会发生调用事件。<br/>
<img src="media/15093533296418/15093742578843.jpg" alt=""/></p></li>
<li><p>时间事件：表明时间段过去，或某个特殊时间点的触发。通过时间表达式是否满足表示事件。用关键字after或when表示。<br/>
<img src="media/15093533296418/15093743310422.jpg" alt=""/></p></li>
<li><p>信号事件：当给定对象收到某实时信号。表示接受一个对象发送的信号(信息)的事件，有可能引发状态迁移。比如抛异常。</p></li>
<li><p><img src="media/15093533296418/15093745491792.jpg" alt=""/></p></li>
</ul>

<p>变更事件与警戒条件的不同：变更会持续监控，而警戒只在出现时调用一次。<br/>
信号与调用区别：信号是异步事件，调用是同步事件。</p>

<p><code>动作</code>Action，是在状态内部或者状态间迁移时执行的原子操作。<br/>
<img src="media/15093533296418/15093747374720.jpg" alt=""/><br/>
订单处理的状态机图：<br/>
<img src="media/15093533296418/15093748772311.jpg" alt=""/><br/>
组合状态：<br/>
<img src="media/15093533296418/15093760055885.jpg" alt=""/><br/>
超级状态：<br/>
<img src="media/15093533296418/15093761079927.jpg" alt=""/></p>

<h2 id="toc_30">3.软件设计与构造</h2>

<h3 id="toc_31">软件体系结构</h3>

<p>软件体系结构包括构成系统的设计元素的描述，设计元素之间的交互，设计元素的组合模式以及在这些模式中的约束。<br/>
MVC就是一个体系结构。<br/>
<img src="media/15093533296418/15093767670193.jpg" alt=""/><br/>
软件体系结构=构建+连接件+约束<br/>
关注于：<br/>
* 如何将复杂的软件系统划分成模块<br/>
* 如果规范模块的构成<br/>
* 如何将这些模块组织成为完整的系统<br/>
* 如何保证系统的质量要求</p>

<p><code>构件</code>是具有某种功能能的可复用的软件结构单元，表示系统中主要的计算元素和数据存储。构件以服务形式存在，以接口与外界交互<img src="media/15093533296418/15093770249845.jpg" alt=""/></p>

<p><code>连接</code>是构件间建立和维护行为关联与信息传递的途径。<img src="media/15093533296418/15093770024886.jpg" alt=""/></p>

<p><code>连接件</code>表示构件之间的交互并实现构件之间的连接。<br/>
<img src="media/15093533296418/15093771638847.jpg" alt=""/></p>

<p><img src="media/15093533296418/15093790901336.jpg" alt=""/></p>

<p><img src="media/15093533296418/15093796059310.jpg" alt=""/><br/>
<img src="media/15093533296418/15093796298964.jpg" alt=""/></p>

<p><code>体系结构风格</code>：用于描述某一特定应用领域中系统组织的惯用模式，反映了 领域中众多系统共有的结构和语义特性。<br/>
<code>设计模式</code>：描述了软件系统设计过程中常见问题的一些解决方案，通常是从大量的成功实践。<br/>
<code>软件框架</code>：软件框架是由开发人员定制的应用系统的股价，是真个或部分系统的可重用设计，鱿鱼足抽象构建和构建实例间的交互方式组成。</p>

<h4 id="toc_32">层次化</h4>

<p><img src="media/15093533296418/15093809665844.jpg" alt=""/></p>

<h4 id="toc_33">软件体系结构风格</h4>

<ul>
<li>独立构建</li>
<li>数据流</li>
<li>数据中心</li>
<li>虚拟机</li>
<li>调用返回</li>
</ul>

<p>下面介绍几个常用风格。</p>

<p><strong>主程序-子程序风格</strong><br/>
<img src="media/15093533296418/15093813679302.jpg" alt=""/><br/>
学习过程的C语言程序就是这种风格。</p>

<p><strong>面向对象风格</strong><br/>
<img src="media/15093533296418/15093814801656.jpg" alt=""/></p>

<p>c++，java程序是这种风格。<br/>
<strong>管道-过滤器风格</strong><br/>
<img src="media/15093533296418/15093816228761.jpg" alt=""/><br/>
<strong>数据中心风格</strong><br/>
<img src="media/15093533296418/15093817810154.jpg" alt=""/><br/>
特点：功能模块之间的交互都通过仓库完成，适合实现经常发生改变，而且具有复杂数据处理的任务。缺点是功能模块与仓库耦合过高，数据存储容易成为性能瓶颈。程序语言编译器，数据库都是仓库结构。</p>

<h3 id="toc_34">接口</h3>

<p><strong>接口是一组操作的集合，它是对类、构建、子系统等行为的抽象</strong><br/>
接口是指对协定进行定义的引用类型。由其他类型实现接口，以保证他们支持某些操作。</p>

<h3 id="toc_35">模块化设计的基本思想及概念</h3>

<h4 id="toc_36">抽象</h4>

<p><code>抽象</code>：关注事物中与问题相关部分而忽略其他无关部分的一种思考方法。</p>

<p>抽象是控制复杂性的基本策略之一，在软件抽象层次中，最高层的抽象程度最高，若需要系统 某部分的细节，就一项较低层次的抽象。越是到较低层次，可看到的细节越多。软件开发过程其实就是对软件抽象层次的一次次细化的的过程。</p>

<ol>
<li>在最高抽象级别上用面向问题域的语言描述问题，概述问题接的形式。</li>
<li>不断具体化，降低抽象级别。</li>
<li>在最低的抽象级别上给出实现问题的解，既源代码。</li>
</ol>

<p>普遍运用的抽象包括三种：<br/>
* 过程抽象：Procedural Abstraction，把完成一个特定功能的动作序列抽象成一个<strong>过程名</strong>和<strong>参数表</strong>，只有指定过程名和实际参数调用此过程，其实就是编程中的<strong>方法</strong>。<br/>
* 数据抽象：Data Abstraction，把一个数据对象的定义抽象为一个数据类型名，用此类型名可定义多个具有相同性质的数据对象，是编程中的<strong>struct</strong>。<br/>
* 对象抽象：Object Abstraction，通过<strong>操作</strong>和<strong>属性</strong>组合了<strong>前两种抽象</strong>。</p>

<h4 id="toc_37">分解</h4>

<p>Decomposition也是控制复杂性的方法。软件设计用分解来实现模块化设计。讲一个复杂的软件系统自顶而下地分解成若干个<code>模块</code>，每个<code>模块</code>完成一个软件的<code>特性</code>，将所有模块组装起来，成为一个整体，完成整个系统所要求的特性。</p>

<p><code>模块</code>是能够单独命名并独立完成一定功能的程序语句的集合，例如结构化语言中的子程序和函数，面型对象语言中的类。</p>

<p><code>模块</code>是可<code>组合</code>，<code>分解</code>，<code>更换</code>的单元。它具有两个基本特征：<br/>
* <code>内部特征</code>：模块内部环境具有的特点。<br/>
* <code>外部特征</code>：模块跟外部环境联系的接口（其他模块调用该模块的方式）和模块的功能。</p>

<p>不使用模块化的大型软件很难被人理解，质量也常常难以保证。</p>

<p>模块本身的复杂度和工作量随着模块的变小而减少，模块的接口工作量却随着模块数的增加而增大，一般把模块控制在最小成本区来减少总开发量。</p>

<p>良好模块化的优点：<br/>
* 降低软件复杂性<br/>
* 提高软件维护性<br/>
* 简化软件的实现</p>

<h4 id="toc_38">封装和信息隐藏</h4>

<p>它是在用来解决<em>如何分解软件来得到最好的一组模块？</em>问题的。</p>

<p>封装和信息隐藏原则：<strong>使一个模块内包含的信息（处理和数据）对于不需要这些信息的模块来说是不能访问的</strong>。提出该原则的目的是为了提高模块的独立性，当修改或者维护模块时减少把一个模块的错误扩散到其他模块中去的机会。</p>

<p>封装和隐藏的是实现细节，将接口和实现有效分离。</p>

<h4 id="toc_39">功能独立</h4>

<p>开发具有独立功能而且与其他模块之间没有过多的相互作用的模块，就可以做到模块独立。</p>

<p><code>内聚</code>是一个模块内各个元素彼此结合的紧密程度，按由低到高包括：</p>

<ul>
<li>偶然内聚：随手把重复代码收集成一个模块了，就出现了偶然内聚的模块。这个模块完成的多个任务之间最多存在比较松散的关系。低内聚。</li>
<li>逻辑内聚：如果一个模块完成的任务在逻辑上术语相同或者相似的一类（比如一个模块产生各种类型的输出），成为逻辑内聚。低内聚。</li>
<li>时间内聚：如果一个模块包含的任务必须在同一段时间内执行（例如模块化完成各种初始化工作）。低内聚。</li>
<li>过程内聚：一个模块内的处理元素是相关的，且必须以特定顺序执行，用流程图确定模块划分得到的往往是过程内聚的模块。中内聚。</li>
<li>通信内聚：如果所有元素都使用同一个输入数据（和，或）产出同一个输出数据。中内聚。</li>
<li>顺序内聚：如果一个模块内的处理元素和同一个功能密切相关，而且这些处理必须顺序执行。根据数据流图划分模块时通常得到顺序内聚的模块，他们的连接往往比较简单。</li>
<li>功能内聚：如果模块内所有元素术语一个整体，完成一个单一的功能。</li>
</ul>

<p>设计应力求高内聚，避免低内聚。</p>

<p><code>耦合</code>是对一个软件结构内不同模块之间互联程度的度量。耦合强弱取决于模块间接口的复杂程度，进入或访问一个模块的点，以及通过接口的数据。 </p>

<p>Myes划分将耦合程度分为七类：</p>

<ul>
<li>非直接耦合：1，2模块间没有消息传递。弱耦合。</li>
<li>数据耦合：1，2被同一模块调用时交换了简单变量。弱耦合。</li>
<li>特征耦合：1，2倍同一模块调用时交换了数据结构。弱耦合。</li>
<li>控制耦合：模块间传递用作控制信号的开关值或者标志量了，就是控制耦合。中等耦合。</li>
<li>外部耦合：允许一组模块访问同一个全局变量。较强耦合。</li>
<li>公共耦合：允许一组模块访问同一个全局性的数据结构。较强耦合。</li>
<li>内容耦合：一个模块可以直接调用另一个模块中的数据。至强耦合！</li>
</ul>

<p>内容耦合是一种<code>病态联系</code>，尽量不要用。</p>

<h3 id="toc_40">面向对象设计原则</h3>

<h4 id="toc_41">开闭原则</h4>

<p>即<strong>OCP</strong><br/>
开闭原则是面向对象设计中<code>可复用设计</code>的基石，是面向对象设计中最重要的原则之一，其他很多的设计原则都是开闭原则的一种手段。</p>

<p><strong>软件实体应该对扩展开放，对修改关闭。</strong></p>

<p>也就是软件系统包含的各种组件，比如<code>模块</code>，<code>类</code>以及<code>功能</code>等，应该在不修改现有代码的基础上引用新功能。</p>

<p>开闭原则中的<code>开</code>是指对于组件功能的扩展是开放的。<code>闭</code>是指对于原有代码的修改是封闭的。</p>

<p>符合开闭原则的优点：<br/>
* 可复用性好。<br/>
* 可维护性好。</p>

<h4 id="toc_42">Liskov替换原则</h4>

<p>既<strong>LSP</strong><br/>
任何基类可以出现的地方，子类一定可以出现。只有当衍生类可以替换掉基类且软件单位功能不受到影响时，基类才能真正被复用，而衍生类也能构在基类的基础上增加新的行为。<br/>
LSP是OCP的扩充。历史代还原则是对实现抽象化具体步骤的规范。</p>

<h4 id="toc_43">依赖转置原则</h4>

<p>A. 高层次的模块不应该依赖于低层次模块，他们都应依赖于抽象。<br/>
B. 抽象不应该依赖于具体实现，具体实现应该依赖于抽象。</p>

<h4 id="toc_44">接口隔离原则</h4>

<p>客户端不应该依赖它不需要的接口；一个类对另一个类的依赖应该建立在最小的接口上。</p>

<ul>
<li>使用多个专门的接口比使用单一的总接口要好。</li>
<li>一个类对另外一个类的依赖应当是建立在最小的接口上的。</li>
</ul>

<p>一个接口代表一个角色，不应当将不同的角色都交给一个接口。没有关系的接口合并在一起，形成一个臃肿的大接口，这是对角色和接口的污染。</p>

<p>解释：如果强迫用户使用它不使用的方法，那么这些客户就会面临由于这些不使用的方法的改变所带来的改变。</p>

<h2 id="toc_45">4.软件测试</h2>

<p><code>测试</code>是为了发现错误而执行程序的过程。</p>

<p>目的：发现程序的错误。<br/>
任务：通过计算机上执行程序，暴露程序中潜在的错误。</p>

<p>测试与调试（Debugging）不同。</p>

<p><code>调试</code>的目的与任务如下：</p>

<p>目的：定位和纠正错误。<br/>
任务：消除软件故障，保证程序的可靠运行。</p>

<p>一般当测试发现软件的错误后通过调试来进行纠正。</p>

<p><code>测试</code>用例是为了某个特殊目标而变质的一组测试输入，执行条件及预期结果，以便测试某个程序路径或者核实是否满足某个特定需求。</p>

<h3 id="toc_46">各种测试概念</h3>

<ul>
<li><code>单元测试</code>是对软件中最小可测单元进行检查和验证。</li>
<li><code>集成测试</code>是通过测试发现与模块接口有关的问题。目标是把通过了单元测试的模块拿来，构造一个在设计中所描述的程序结构，应当避免一次性的集成（除非软件规模很小），而采用增量集成。</li>
<li><code>系统测试</code>是将经过集成测试的软件，作为计算机系统的一个部分，与系统中其他部分结合起来，在实际运行环境下对计算机系统进行的一系列严格有效地测试，以发现软件潜在的问题，保证系统的正常运行。</li>
<li><code>回归测试</code>是回归测试是指重复以前的全部或部分的相同测试。当发现并修改缺陷后，或在软件中添加新的功能后，重新测试。用来检查被发现的缺陷是否被改正，并且所做的修改没有引发新的问题。</li>
<li><code>确认测试</code>是在模拟的环境下，运用黑盒测试的方法，验证被测软件是否满足需求规格说明书列出的需求。</li>
</ul>

<h3 id="toc_47">单元测试</h3>

<p><code>单元测试</code>是对软件中最小可测单元进行检查和验证。<br/>
优点：<br/>
* 验证代码：能不能跑通<br/>
* 设计更好：从使用者角度看<br/>
* 文档化行为：做Demo<br/>
* 具有回归性：自动化的可以做回归测试</p>

<p>内容包括：<br/>
* 模块接口：对通过所有北侧模块的数据流进行测试。<br/>
* 局部数据结构：检查模块中的数据结构是否正确的定义和使用。<br/>
* 边界条件：检查数据流或控制流中条件或数据处于边界时的出错可能性。<br/>
* 独立路径：检查由于计算错误、判定错误、控制流错误导致的程序错误。<br/>
* 出错处理：检查可能引发错误处理的路径以及进行错误处理的路径。</p>

<p>原则：<br/>
* 快速的<br/>
* 独立的<br/>
* 可重复的<br/>
* 自我验证的<br/>
* 及时的</p>

<p>过程是：<img src="media/15093533296418/15093891125726.jpg" alt=""/></p>

<p>衡量测试质量的指标：</p>

<p><code>测试通过率</code>是指在测试过程中执行通过的测试用例所占比例，单元测试通常要求测试用例通过率达到100%。</p>

<p><code>测试覆盖率</code>是用来度量测试完整性的一个手段，通过覆盖率数据，可以了解测试是否充分以及弱点在哪里。<code>代码覆盖率</code>是单元测试的一个衡量标准，但也不能一味地追求覆盖率。一般要求达到70%~80%。</p>

<p>单元测试分为：<br/>
* <code>静态测试</code>：通过人工分析或程序正确性证明的方式来确认程序的正确性。<br/>
* <code>动态测试</code>：通过动态分析和程序测试等方法来检查和确认程序是否有问题。</p>

<p><em>动态与静态的区别在于电脑要不要跑这个程序~</em></p>

<p>基于测试用例设计，把单元测试分为：<br/>
* <code>黑盒测试</code>：又称功能测试，它将测试对象看做一个黑盒子，完全不考虑程序内部的逻辑结构和内部特性，只根据需求规格说明书，检查程序的功能是否符合它的功能说明。<br/>
* <code>白盒测试</code>：又称结构测试，它把测试对象看做一个透明的盒子，允许测试人员利用程序内部的逻辑结构及有关信息，设计或选择测试用例，对程序所有逻辑路径进行测试。<br/>
<code>驱动模块</code>：单元测试中用于替代上层模块调用。<br/>
<code>桩模块</code>：单元测试中用于替代下层调用模块。</p>

<p>xUnit工具适用的场景：<br/>
* 单个函数，一个类或者几个功能相关类的测试<br/>
* 特别适合春函数或者接口级别的测试</p>

<p>不适用的复杂场景：<br/>
* 被测对象依赖关系复杂，甚至无法简单创建。<br/>
* 对于一些失败场景的测试<br/>
* 被测对象中涉及多线程合作<br/>
* 被测对象通过消息与外接交互</p>

<p><code>Mock</code>测试是在测试过程中对于某些不容易构造或者不容易获取的对象，用一个虚拟的对象（既Mock对象）老创建以便于测试的方法。<br/>
Mock技术的关键是需要应用<code>针对接口</code>的编程技术，既被测试的代码通过接口来引用对象，再使用Mock对象模拟所引用的对象及其行为，因此被测试模块并不知道它所引用的究竟是真是对象还是Mock对象。这样可以把北侧模块和所以来的模块进行隔离。<br/>
<img src="media/15093533296418/15093909721979.jpg" alt=""/><br/>
这个用例很实际，是不是应该把外部的调用都通过Interface封装下？</p>

<h4 id="toc_48">黑盒测试</h4>

<p>设计良好的测试用例是测试的关键。它可以：<br/>
* 降低软件测试成本<br/>
* 保证测试工作质量<br/>
* 评估和检验测试效果</p>

<p><code>测试用例</code>是为了一个特定的目标而设计的一组<code>测试输入</code>，<code>执行条件</code>，<code>预期结果</code>。它的目的是测试某个程序<code>路径是否正确</code>或者<code>核实程序是否满足某个特定需求</code>。</p>

<p><img src="media/15093533296418/15093912959231.jpg" alt=""/><br/>
测试用例设计应该考虑：<br/>
* 少量代表性和典型性<br/>
* 寻求系统设计和功能设计的弱点<br/>
* 既有正确输入也有错误或异常输入<br/>
* 考虑用户实际的诸多使用场景</p>

<p><code>等价类划分</code>是将输入域划分为尽可能少的若干紫玉，在划分中要求每个子域两两互不相交，每个子域成为一个等价类。<br/>
<img src="media/15093533296418/15093916652235.jpg" alt=""/><br/>
<code>有效等价类</code>是对规格说明有意义，合理的输入数据构成的集合，能够校验程序是否实现了规格说明中预先规定的功能和性能。<br/>
<code>无效等价类</code>是对规格说明无异议，不合理的输入数据构成的集合，已检查程序是否具有一定的容错性。<br/>
一个例子：<br/>
<img src="media/15093533296418/15093919267744.jpg" alt=""/><br/>
另一个例子：<br/>
<img src="media/15093533296418/15093920141102.jpg" alt=""/></p>

<p>再来个例子：<br/>
<img src="media/15093533296418/15093920712867.jpg" alt=""/><br/>
还是例子：<br/>
<img src="media/15093533296418/15093921282305.jpg" alt=""/></p>

<p><code>复合数据类型</code>是包含两个或两个以上相互独立的属性的输入数据，在进行等价类划分时需要考虑输入数据的每个属性的合法和非法取值（所有组合）。<br/>
<img src="media/15093533296418/15093923192099.jpg" alt=""/><br/>
<img src="media/15093533296418/15093924180096.jpg" alt=""/><br/>
多数情况根据输入域划分，有一些情况可以用输出域划分。<br/>
<img src="media/15093533296418/15093924679759.jpg" alt=""/><br/>
<code>边界值分析</code>是对输入或输出的边界值进行测试的一种方法，它通常作为等价类划分法的补充，这种情况下的测试用例来自等价类的边界。<br/>
边界值分析的<code>步骤</code>是：<br/>
* 先确定边界：通常输入或输出等价类的边界就是应该着重测试的边界情况。<br/>
* 选取正好等于，刚好大于或刚刚小于边界的值作为测试数据。<br/>
<img src="media/15093533296418/15093926521127.jpg" alt=""/><br/>
<code>故障往往出现在输入变量的边界值附近</code><br/>
<img src="media/15093533296418/15093927012589.jpg" alt=""/><br/>
<code>健壮性测试</code>是作为边界值分析的一个简单的扩充，它除了对变量的5个边界值分析取值外，还要哦增加一个略大于最大值及略小于最小值的取值，检查超过极限值是系统的情况。<br/>
<img src="media/15093533296418/15093928124381.jpg" alt=""/><br/>
<code>错误推测发</code>是人们根据经验或者直觉推测程序中可能存在的各种错误，从而有针对性第便携检查这些错误的测试用例的方法。</p>

<h4 id="toc_49">白盒测试</h4>

<p><code>测试需求</code>：测试需求是软件制品的一个<code>特定元素</code>，测试用例必须<code>满足</code>或<code>覆盖</code>这个特定元素。</p>

<p><code>覆盖标准</code>：一个覆盖标准是一条规则，或者将测试需求施加在一个测试机上的一组规则。（例如每一条语句都要执行）</p>

<p><code>测试覆盖</code>：给定一个覆盖标准C和相关的测试需求集合TR，如果说一个测试集合T满足C，那么对于测试集合TR中的每一条测试需求tr，在T中至少存在一个测试t可以满足tr。(这个不是覆盖率，是覆盖，绝对的包含关系，每一个测试需求，至少有一个测试用例满足)</p>

<p><code>覆盖程度</code>：给定一个测试需求集合TR和一个测试集合T，覆盖程度就是T满足测试需求数栈TR总数的比例。</p>

<ul>
<li>软件缺陷具有空间聚集性（比如项*的路径）
代码覆盖率一般包括：</li>
<li>语句覆盖</li>
<li>判定覆盖</li>
<li>条件覆盖</li>
<li>判定条件覆盖</li>
<li>条件组合覆盖</li>
<li>路径覆盖</li>
</ul>

<p>面包机的例子<img src="media/15093533296418/15093940538247.jpg" alt=""/><br/>
单纯的黑盒测试时不够的，还要使用白盒测试，白盒测试对代码中的逻辑关系的覆盖更为全面，做得好可以及大地增强产品的<code>健壮性</code>。</p>

<p><code>控制流图</code>Control Flow Graph是一个过程或程序的抽象表示。是流程图的一个简化。<br/>
* 矩形代表连续的顺序计算，也称为基本快。<br/>
* 节点是语句或语句的一部分，便表示语句的控制流。<br/>
<img src="media/15093533296418/15093943037631.jpg" alt=""/></p>

<p><img src="media/15093533296418/15093944472409.jpg" alt=""/></p>

<h2 id="toc_50">Tip</h2>

<p>面向过程的开发，上层调用下层，上层依赖于下层，当下层剧烈变动时上层也要跟着变动，这就会导致模块的复用性降低而且大大提高了开发的成本。</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">10/30/2017 16:48 下午</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='note.html'>课程笔记</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15089123496348.html">
                
                  <h1>数据结构笔记</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">线性表</h2>

<h3 id="toc_1">顺序存储结构</h3>

<p>顺序结构优点<br/>
顺序结构缺点：<br/>
稀疏空间浪费<br/>
增删慢</p>

<h3 id="toc_2">向量</h3>

<p>向量是线性数组的一种抽象与泛化，它是具有线性次序的一组元素构成的集合V={\(v_1,v_2,...,v_n\)}，其中的元素分别由秩相互区分。可以有多种实现方式。<br/>
ADT：<br/>
    size()<br/>
    get(r)<br/>
    put(r,e)<br/>
    insert(r,e)<br/>
    remove(r)<br/>
    disordered()#是否按非降序排列<br/>
    sort()#按非降序排列<br/>
    find(e)#查找等于e且秩最大的元素</p>

<h2 id="toc_3">图</h2>

<h3 id="toc_4">基本概念</h3>

<p>图表示多对多的关系。</p>

<p>基本元素包括<code>顶点</code>，和<code>边</code>。</p>

<p>一个图包含V={\(v_1,v_2,...\)}和E={\(e_1,e_2,...\)}</p>

<p><strong>连通</strong>:如果从V到W存在一条(无向)路径，则称 V和W是连通的</p>

<p><strong>路径</strong>:V到W的路径是一系列顶点{V, v1, v2, ..., vn, W}的集合，其中任一对相邻的顶点间都有图<br/>
中的边。路径的长度是路径中的边数(如果带权，则是所有边的权重和)。如果V到W之间的所<br/>
 有顶点都不同，则称简单路径。</p>

<p><strong>回路</strong>:起点等于终点的路径（回路指的是路径）</p>

<p><strong>连通图</strong>:图中任意两顶点均连通</p>

<p><strong>连通分量</strong>:无向图的极大连通子图</p>

<p><strong>极大联通子图</strong><br/>
* 极大顶点数:再加1个顶点就不连通了<br/>
* 极大边数:包含子图中所有顶点相连的所有边</p>

<p><strong>强连通</strong>:有向图中顶点V和W之间存在双向路 径，则称V和W是强连通的<br/>
<strong>强连通图</strong>:有向图中任意两顶点均强连通<br/>
<strong>强连通分量</strong>:有向图的极大强连通子图</p>

<h3 id="toc_5">图的存储</h3>

<p>两种方法：</p>

<h4 id="toc_6">邻接矩阵：直接用二维矩阵存储。<img src="media/15089123496348/15091340290429.jpg" alt=""/></h4>

<p>优点：<br/>
* 简单，容易理解。<br/>
* 方便检查两个点之间是否有边。<br/>
* 方便找出任意顶点的“邻接点”（有边直接相连的点）。<br/>
* 方便计算任一顶点的“度”(从该点发出的边数为“出度”，指向该点的边数为“入度”)</p>

<p>缺点：<br/>
* 存稀疏图比较浪费空间。<br/>
* 统计边数比较浪费时间。</p>

<h4 id="toc_7">邻接表</h4>

<p><img src="media/15089123496348/15091342563875.jpg" alt=""/></p>

<p>优点：<br/>
* 方便计算出度。<br/>
* 方便查找“邻接点”。<br/>
* 节约稀疏图的存储空间，需要V个头指针和2E个节点。<br/>
* 方便计算某一顶点的度（有向图需要逆邻接表）</p>

<p>缺点：<br/>
* 判断两个节点之间是否有边变麻烦了。</p>

<h3 id="toc_8">图的遍历</h3>

<h4 id="toc_9">深度优先遍历</h4>

<p>Depth First Search<br/>
时间复杂度为：<br/>
<strong>邻接表</strong>：\(O(N+E)\)<br/>
<strong>邻接矩阵</strong>：\(O(N^2)\)<br/>
只要不是太稠密都是邻接表快一些，根本就是邻接表快一些！！因为E最大等于n（n-1）/2</p>

<p><img src="media/15089123496348/15091396026315.jpg" alt=""/><br/>
常用DFS来扫描极大连通分量。<br/>
<img src="media/15089123496348/15091402931171.jpg" alt=""/></p>

<h4 id="toc_10">广度优先算法</h4>

<p>Breadth First Search</p>

<p>复杂度和深度优先一样。<br/>
<img src="media/15089123496348/15091398927830.jpg" alt=""/></p>

<h4 id="toc_11">广度优先遍历</h4>

<h3 id="toc_12">Dijkstra算法</h3>

<p>用于计算<strong>有权图</strong>的<strong>单源最短路径</strong>。<br/>
<img src="media/15089123496348/15091410200093.jpg" alt=""/><br/>
假设的证明：<br/>
v是马上要收录的顶点，如果存在未包含的顶点w，使s-w-v距离小于s-v，那么|s-w|&lt;|s-v|,那么s-w应早于s-v收录。</p>

<p>收集顶点应按权值升序收集。<br/>
<img src="media/15089123496348/15091415783330.jpg" alt=""/><br/>
说明，dist数组用来存储源s到v的最短路径，path则是上一个节点的索引值。<br/>
所谓收录其实是未收录节点的反。 <br/>
根据收集最小权值顶点和距离更新方法的不同算法复杂度不同。</p>

<p>主要是未收录的如何存储，未收录的可以用任何线性结构，存储存在距离的节点，动态添加和删除。 </p>

<ul>
<li>直接扫描收集：T=\(O(|V|^2+|E|)\),对于稠密图效果更好。</li>
</ul>

<p>把dist存在最小堆里：\(O(\log(|E|\log|V|))\)，对于稀疏图效果更好（充分稀疏，边与顶点同数量级）。</p>

<p><strong>对于稀疏矩阵，可以直接用dijkstra算法计算多源最短路径</strong>（稀疏矩阵用线性结构存储未收集表）</p>

<h3 id="toc_13">Floyd算法</h3>

<p>初始距离矩阵是邻接矩阵。<br/>
直接在邻接矩阵上迭代。<br/>
<img src="media/15089123496348/15091442751084.jpg" alt=""/></p>

<p><img src="media/15089123496348/15091442524383.jpg" alt=""/></p>

<p>这个可以画图看下，用那个推论就比较容易理解了。</p>

<h3 id="toc_14">Prime算法</h3>

<p>是一个最小生成树算法，小树长大，跟Dijkstra比较像，都是一个一个收率。例子的话就是修路问题。<br/>
查找dist最小时用数组，因为这个算法在稠密图中比较有效。</p>

<p>复杂度\(O(|V|^2)\)</p>

<p><img src="media/15089123496348/15091446428843.jpg" alt=""/><br/>
<img src="media/15089123496348/15091451816636.jpg" alt=""/></p>

<h3 id="toc_15">Kruskal算法</h3>

<p>也是一个最小生成树算法，把森林合成树。每个节点都作为一个树，贪婪合并树直到边数到|V|-1或是边用完了。<br/>
<img src="media/15089123496348/15091455629904.jpg" alt=""/><br/>
复杂度T=\(O(|E|log|E|)\),当稀疏时效率高于Prime</p>

<h3 id="toc_16">拓扑排序</h3>

<p>AOV：Activity On Vertex，指活动在顶点上表示的图。</p>

<p>拓扑序：如果图中从V到W有一条有向路径，则V一定排在W之前。满足此条件的顶点序列称为一个拓扑序。</p>

<p>拓扑排序：获得一个拓扑序的过程。</p>

<p>DAG：有向无环图 Directed Acyclic Graph。</p>

<p>AOV一定是DAG</p>

<p><img src="media/15089123496348/15091460902471.jpg" alt=""/><br/>
就是不断把入度为0的节点提出来并删除这个节点指向其他节点的边（优化是记录入度，如果目标节点入度减至0就把那个节点扔到下次迭代要提出的队列里去）。迭代至队列里没顶点，如果没pop出|V|个节点，那就是有回路。</p>

<p>复杂度是\(O(|V|+|E|)\)</p>

<h2 id="toc_17">排序</h2>

<h3 id="toc_18">冒泡排序</h3>

<p>冒牌排序尾部迭代N次后尾部N个不需要再排了。</p>

<p>最好O（N）<br/>
最差O（\(N^2\)）</p>

<p>优点是简单，可以排单链表。是稳定的。</p>

<h3 id="toc_19">插入排序</h3>

<p>计算过程复用了原来的数组</p>

<p>最好O（N）<br/>
最差O（\(N^2\)）</p>

<p>优点是简单，步骤比较省，更好的性质后边说，也是稳定的。</p>

<h3 id="toc_20">希尔排序</h3>

<p>使用递减级数抽取子数列做插入排序，使用不同增量数列效果不同。同样比较简单,不稳定。<br/>
<img src="media/15089123496348/15091332165058.jpg" alt=""/></p>

<h3 id="toc_21">堆排序</h3>

<p>两种实现：<br/>
* 从最小堆里一个一个Pop出来。需要额外O（N）空间。复制元素需要时间。<br/>
* 重复调整最大堆，把堆顶元素与末尾元素调换位置，减少规模，迭代下去~。这里的堆排序索引需要特殊处理，下表计算与堆章节下表计算公式不同，考试的时候画一下。<br/>
<img src="media/15089123496348/15091335578311.jpg" alt=""/></p>

<h3 id="toc_22">快速排序</h3>

<p><img src="media/15089123496348/15091327687146.jpg" alt=""/></p>

<ol>
<li>将[head,(head+tail)/2,tail]升序排列</li>
<li>swap（(head+tail)/2,tail-1）</li>
<li>使用两个指针对[head:tail-2]遍历swap</li>
<li>把tail-1插到左侧出发指针所在位置i</li>
<li>对i两侧区间递归排序</li>
</ol>

<h3 id="toc_23">基数排序</h3>

<p>就是把要排序的值分桶，有不同的分桶方法，对于多位的值还有不同的顺序（后序优点&amp;&amp;先序优先）<br/>
<img src="media/15089123496348/15091331445630.jpg" alt=""/></p>

<p>多关键字就再来一次分桶收集。</p>

<h3 id="toc_24">归并排序</h3>

<p><img src="media/15089123496348/15091331212930.jpg" alt=""/></p>

<p>归并，O（N ）<br/>
O（NlogN），非常稳定，最好最坏一样。稳定的。非常适合外排序中。</p>

<p>分为递归实现和非递归实现。因为占用空间较大，一般不用于内排序。<br/>
这个非递归实现回头画图看下</p>

<h3 id="toc_25">排序总结</h3>

<table>
<thead>
<tr>
<th>排序方法</th>
<th>平均时间复杂度</th>
<th>最坏时间复杂度</th>
<th>额外空间复杂度</th>
<th>稳定性</th>
<th>特点</th>
</tr>
</thead>

<tbody>
<tr>
<td>选择排序</td>
<td>\(O(N^2)\)</td>
<td>\(O(N^2)\)</td>
<td>O(1)</td>
<td>不稳定</td>
<td></td>
</tr>
<tr>
<td>冒泡排序</td>
<td>\(O(N^2)\)</td>
<td>\(O(N^2)\)</td>
<td>O(1)</td>
<td>稳定</td>
<td></td>
</tr>
<tr>
<td>插入排序</td>
<td>\(O(N^2)\)</td>
<td>\(O(N^2)\)</td>
<td>O(1)</td>
<td>稳定</td>
<td></td>
</tr>
<tr>
<td>希尔排序</td>
<td>\(O(N^d)\)</td>
<td>\(O(N^2)\)</td>
<td>O(1)</td>
<td>不稳定</td>
<td></td>
</tr>
<tr>
<td>堆排序</td>
<td>\(O(N_{log}N)\)</td>
<td>\(O(N_{log}N)\)</td>
<td>O(1)</td>
<td>不稳定</td>
<td>常数比较大</td>
</tr>
<tr>
<td>快速排序</td>
<td>\(O(N_{log}N)\)</td>
<td>\(O(N^2)\)</td>
<td>O(logN)</td>
<td>不稳定</td>
<td>常数比较小，需要额外空间</td>
</tr>
<tr>
<td>归并排序</td>
<td>\(O(N_{log}N)\)</td>
<td>\(O(N_{log}N)\)</td>
<td>O(N)</td>
<td>稳定</td>
<td>占用空间大，</td>
</tr>
<tr>
<td>基数排序</td>
<td>\(O(P(N+B))\)</td>
<td>\(O(P(N+B))\)</td>
<td>O(N+B)</td>
<td>稳定</td>
<td>速度看桶，场景有限</td>
</tr>
</tbody>
</table>

<p><em>P是多少次，B是多少个桶</em></p>

<p><strong>归并排序</strong>是<strong>复杂排序算法</strong>中唯一一个<strong>稳定的</strong>。<br/>
<strong>选择排序</strong>是<strong>简单排序算法</strong>中唯一一个<strong>不稳定的</strong>。<br/>
<strong>冒泡排序</strong>可以用于<strong>单链表</strong>。<br/>
<strong>堆排序</strong>常数比较大。<br/>
<strong>快速排序</strong>常数比较小，非常快。<br/>
<strong>基数排序</strong>比较适合<em>容易分桶的</em></p>

<h2 id="toc_26">散列</h2>

<p>成功平均查找长度：ASLs</p>

<p>要查找的对象在表里。</p>

<p>失败平均查找长度：ASLu</p>

<p>要查找的对象不在表里。 </p>

<p><img src="media/15089123496348/15091249486795.jpg" alt=""/></p>

<h3 id="toc_27">散列冲突处理方法</h3>

<h4 id="toc_28">开放地址法</h4>

<p>发现冲突时按某种规则放到其他位置上。</p>

<p>发生第i词冲突时，试探下一个地址增加\(d_i\),基本公式是：\(h_i(key)=(h(key)+d_i)\% TableSize,1\le i\lt TableSize\)</p>

<p><strong>优点</strong></p>

<p>散列表是一个数组，存储效率高，随机查找。</p>

<p><strong>缺点</strong></p>

<p>散列表有聚集现象 </p>

<p>注意次数是从1开始的，判定没找到那个空也算次数里。</p>

<p><strong>线性探测</strong>：</p>

<pre><code>增量序列 1，2，3，4，5....
</code></pre>

<p>聚集问题比较严重</p>

<p><strong>平方探测</strong>：<br/>
增量序列为：\(d_i=(-ceil(\frac{i}{2}))^{i-1}\)<br/>
就是 1，-1，4，-4，9，-9，...</p>

<p>有效避免了聚集问题，但如果TableSize没有设好容易出现探测不到闲置空间的情况。</p>

<p>定理：<code>如果散列表长度是某个4k+3形式的素数时，平方探测法可以谈查到整个散列表空间</code></p>

<p><strong>双散列探测法</strong>：</p>

<p>\(d_i\)是\(i*h_2(key),h_2(key)\)是另一散列函数</p>

<p>探测序列成：\(h_2(key),2h_2(key),3h_2(key),...\)<br/>
对任意key，\(h_2(key)\ne 0\)<br/>
<img src="media/15089123496348/15091271513853.jpg" alt=""/></p>

<h4 id="toc_29">再散列</h4>

<p>装填因子过大时，查找效率会下降，实用装填因子一般取0.5~0.85，超过超过0.5过多时一般要扩大散列表，新建一个更大的表（还是素数长度），把原来表里的东西都丢进去。</p>

<h4 id="toc_30">链地址法</h4>

<p>就是value不是元素而是元素链表啦。</p>

<p><strong>优点</strong>：<br/>
容易实现，关键字删除不需要<code>懒惰删除法</code>没有存储<code>垃圾</code>。<br/>
<strong>缺点</strong>：<br/>
* 链表部分存储效率和查找效率都比较低。<br/>
* \(\alpha\)小的时间代价高，大则可能导致空间浪费，不均匀的链表长度导致时间效率的严重下降。</p>

<h4 id="toc_31">性能分析</h4>

<p>关键词的比较次数，取决于产生冲突的多少。<br/>
映像产生冲突多少有以下三个元素：<br/>
* 散列函数是否均匀<br/>
* 处理冲突的方法<br/>
* 散列表的 装填因子\(\alpha\) </p>

<p><strong>线性探测法</strong><br/>
<img src="media/15089123496348/15091289632960.jpg" alt=""/></p>

<p><strong>平方探测法和双散列法</strong><br/>
<img src="media/15089123496348/15091290709161.jpg" alt=""/></p>

<p><strong>分离链表法</strong></p>

<p>特点：<br/>
散列表是顺序存储和</p>

<p><img src="media/15089123496348/15091295097144.jpg" alt=""/></p>

<h4 id="toc_32">散列查找的特点</h4>

<p><strong>优点</strong>：<br/>
散列的查找效率期望为常数O(1)，几乎与大小无关。适合关键字直接比较量大的问题，常用于字符串管理。</p>

<p><strong>特点</strong>：<br/>
以较小的\(\alpha\)为前提，空间换时间。</p>

<p><strong>缺点</strong>：<br/>
散列方法的存储对关键字是随机的，不便于<code>顺序查找关键字</code>，也不适合于<code>范围查找</code>，或<code>最大值最小值</code>查找。</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">10/25/2017 14:19 下午</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='note.html'>课程笔记</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15077052334055.html">
                
                  <h1>数据结构随堂笔记</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">线性结构</h2>

<h3 id="toc_1">线性表</h3>

<p>视频中例子是多项式表达。</p>

<p>顺序存储（数组表示）:a[i]是系数，i是指数。缺点是稀疏时比较浪费空间。</p>

<p>结构数组:a[i]是一个元组（\(i_k,n_k\)）分别是系数和指数。</p>

<p>链表结构:和上面的一样也是存元组。</p>

<p><strong>线性表定义</strong>：由同类型<strong>数据元素</strong>构成的<strong>有序序列</strong>的线性结构。定义了【长度，空表，表头，表尾】</p>

<h4 id="toc_2">ADT:</h4>

<ul>
<li><strong>List MakeEmpty()</strong>  创建一个新的线性表。</li>
<li><strong>T FindKth(int K,List L)</strong>  找到表中的第K个元素。</li>
<li><strong>int Find(T X,List L)</strong> 找到第一个X出现的位置。</li>
<li><strong>void insert(T X,int i,List L)</strong>在位序i前插入一个新元素X。</li>
<li><strong>void Delete(int i,List L)</strong> 删除指定位置元素。</li>
<li><strong>int Length(List L)</strong> 返回线性表L的长度n。</li>
</ul>

<h4 id="toc_3">实现：</h4>

<p>1.顺序存储 <br/>
2.链表存储</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">10/11/2017 15:0 下午</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='doc.html'>文档</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15075982914933.html">
                
                  <h1>数学 10_10 微分方程</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>球形体积公式：<br/>
\[V=\frac{4}{3}{\pi}R^3\]</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">10/10/2017 9:18 上午</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='course.html'>课程</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15039097733460.html">
                
                  <h1>Faster RCNN</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">1.简介</h2>

<p>Recent advances in object detection are driven by the success of region proposal methods (e.g., [4]) and region-based convolutional neural networks (R- CNNs) [5]. Although region-based CNNs were com- putationally expensive as originally developed in [5], their cost has been drastically reduced thanks to shar- ing convolutions across proposals [1], [2]. The latest incarnation, Fast R-CNN [2], achieves near real-time rates using very deep networks [3], when ignoring the time spent on region proposals. Now, proposals are the test-time computational bottleneck in state-of-the-art detection systems.</p>

<p>Region proposal methods typically rely on inex- pensive features and economical inference schemes. Selective Search [4], one of the most popular meth- ods, greedily merges superpixels based on engineered low-level features. Yet when compared to efficient detection networks [2], Selective Search is an order of magnitude slower, at 2 seconds per image in a CPU implementation. EdgeBoxes [6] currently provides the best tradeoff between proposal quality and speed, at 0.2 seconds per image. Nevertheless, the region proposal step still consumes as much running time as the detection network.</p>

<p>One may note that fast region-based CNNs take advantage of GPUs, while the region proposal meth- ods used in research are implemented on the CPU, making such runtime comparisons inequitable. An ob- vious way to accelerate proposal computation is to re- implement it for the GPU. This may be an effective en- gineering solution, but re-implementation ignores the down-stream detection network and therefore misses important opportunities for sharing computation.<br/>
In this paper, we show that an algorithmic change— computing proposals with a deep convolutional neu- ral network—leads to an elegant and effective solution where proposal computation is nearly cost-free given the detection network’s computation. To this end, we introduce novel Region Proposal Networks (RPNs) that share convolutional layers with state-of-the-art object detection networks [1], [2]. By sharing convolutions at test-time, the marginal cost for computing proposals is small (e.g., 10ms per image).</p>

<p>Our observation is that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region pro- posals. On top of these convolutional features, we construct an RPN by adding a few additional con- volutional layers that simultaneously regress region bounds and objectness scores at each location on a regular grid. The RPN is thus a kind of fully convo- lutional network (FCN) [7] and can be trained end-to- end specifically for the task for generating detection proposals.</p>

<p>RPNs are designed to efficiently predict region pro- posals with a wide range of scales and aspect ratios. In contrast to prevalent methods [8], [9], [1], [2] that use pyramids of images (Figure 1, a) or pyramids of filters (Figure 1, b), we introduce novel “anchor” boxes that serve as references at multiple scales and aspect ratios. Our scheme can be thought of as a pyramid of regression references (Figure 1, c), which avoids enumerating images or filters of multiple scales or aspect ratios. This model performs well when trained and tested using single-scale images and thus benefits running speed.</p>

<p>To unify RPNs with Fast R-CNN [2] object detec- tion networks, we propose a training scheme that alternates between fine-tuning for the region proposal task and then fine-tuning for object detection, while keeping the proposals fixed. This scheme converges quickly and produces a unified network with convo- lutional features that are shared between both tasks.1</p>

<p>We comprehensively evaluate our method on the PASCAL VOC detection benchmarks [11] where RPNs with Fast R-CNNs produce detection accuracy bet- ter than the strong baseline of Selective Search with Fast R-CNNs. Meanwhile, our method waives nearly all computational burdens of Selective Search at test-time—the effective running time for proposals is just 10 milliseconds. Using the expensive very deep models of [3], our detection method still has a frame rate of 5fps (including all steps) on a GPU, and thus is a practical object detection system in terms of both speed and accuracy. We also report results on the MS COCO dataset [12] and investi- gate the improvements on PASCAL VOC using the COCO data. Code has been made publicly available at <a href="https://github.com/shaoqingren/faster_">https://github.com/shaoqingren/faster_</a> rcnn (in MATLAB) and <a href="https://github.com/">https://github.com/</a> rbgirshick/py-faster-rcnn (in Python).</p>

<p><img src="media/15039097733460/15039100798184.jpg" alt="" title="Figure 1: Different schemes for addressing multiple scales and sizes. (a) Pyramids of images and feature maps are built, and the classifier is run at all scales. (b) Pyramids of filters with multiple scales/sizes are run on the feature map. (c) We use pyramids of reference boxes in the regression functions."/></p>

<p>A preliminary version of this manuscript was pub- lished previously [10]. Since then, the frameworks of RPN and Faster R-CNN have been adopted and gen- eralized to other methods, such as 3D object detection [13], part-based detection [14], instance segmentation [15], and image captioning [16]. Our fast and effective object detection system has also been built in com-mercial systems such as at Pinterests [17], with user engagement improvements reported.</p>

<p>In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the basis of several 1st-place entries [18] in the tracks of ImageNet detection, Ima- geNet localization, COCO detection, and COCO seg- mentation. RPNs completely learn to propose regions from data, and thus can easily benefit from deeper and more expressive features (such as the 101-layer residual nets adopted in [18]). Faster R-CNN and RPN are also used by several other leading entries in these competitions2. These results suggest that our method is not only a cost-efficient solution for practical usage, but also an effective way of improving object detec- tion accuracy.</p>

<h2 id="toc_1">2. 相关工作</h2>

<p><strong>Object Proposals</strong>. There is a large literature on object proposal methods. Comprehensive surveys and com- parisons of object proposal methods can be found in [19], [20], [21]. Widely used object proposal methods include those based on grouping super-pixels (e.g., Selective Search [4], CPMC [22], MCG [23]) and those based on sliding windows (e.g., objectness in windows [24], EdgeBoxes [6]). Object proposal methods were adopted as external modules independent of the de- tectors (e.g., Selective Search [4] object detectors, R- CNN [5], and Fast R-CNN [2]).</p>

<p><strong>Deep Networks for Object Detection</strong>. The R-CNN method [5] trains CNNs end-to-end to classify the proposal regions into object categories or background. R-CNN mainly plays as a classifier, and it does not predict object bounds (except for refining by bounding box regression). Its accuracy depends on the perfor- mance of the region proposal module (see compar- isons in [20]). Several papers have proposed ways of using deep networks for predicting object bounding boxes [25], [9], [26], [27]. In the OverFeat method [9], a fully-connected layer is trained to predict the box coordinates for the localization task that assumes a single object. The fully-connected layer is then turned into a convolutional layer for detecting multiple class- specific objects. The MultiBox methods [26], [27] gen- erate region proposals from a network whose last fully-connected layer simultaneously predicts mul- tiple class-agnostic boxes, generalizing the “single- box” fashion of OverFeat. These class-agnostic boxes are used as proposals for R-CNN [5]. The MultiBox proposal network is applied on a single image crop or multiple large image crops (e.g., 224×224), in contrast to our fully convolutional scheme. MultiBox does not share features between the proposal and detection networks. We discuss OverFeat and MultiBox in more depth later in context with our method. Concurrent with our work, the DeepMask method [28] is devel- oped for learning segmentation proposals.</p>

<p>hared computation of convolutions [9], [1], [29], [7], [2] has been attracting increasing attention for ef- ficient, yet accurate, visual recognition. The OverFeat paper [9] computes convolutional features from an image pyramid for classification, localization, and de- tection. Adaptively-sized pooling (SPP) [1] on shared convolutional feature maps is developed for efficient region-based object detection [1], [30] and semantic segmentation [29]. Fast R-CNN [2] enables end-to-end detector training on shared convolutional features and shows compelling accuracy and speed.</p>

<h2 id="toc_2">3.Faster RCNN</h2>

<p>Our object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology of neural networks with ‘attention’ [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of the network for region proposal. In Section 3.2 we develop algorithms for training both modules with features shared.</p>

<p>我们的目标识别系统，叫做Faster R-CNN，是两个模组的组合。第一个模块是一个用来做猜测的深度全连接网络，第二个木块是一个使用猜测区域的 Fast-RCNN检测器。系统入口是一个dan</p>

<h3 id="toc_3">3.1 Region Proposal Networks</h3>

<p>A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.3 We model this process with a fully convolutional network [7], which we describe in this section. Because our ulti- mate goal is to share computation with a Fast R-CNN object detection network [2], we assume that both nets share a common set of convolutional layers. In our ex- periments, we investigate the Zeiler and Fergus model <a href="ZF">32</a>, which has 5 shareable convolutional layers and the Simonyan and Zisserman model <a href="VGG-16">3</a>, which has 13 shareable convolutional layers.<br/>
To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer. This small network takes as input an n × n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU [33] following). This feature is fed into two sibling fully- connected layers—a box-regression layer (reg) and a box-classification layer (cls). We use n = 3 in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively). This mini-network is illustrated at a single position in Figure 3 (left). Note that be- cause the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations. This architecture is naturally im- plemented with an n×n convolutional layer followed by two sibling 1 × 1 convolutional layers (for reg and cls, respectively).</p>

<h4 id="toc_4">3.1.1 Anchors</h4>

<p>At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as k. So the reg layer has 4k outputs encoding the coordinates of k boxes, and the cls layer outputs 2k scores that estimate probability of object or not object for each proposal4. The k proposals are param- eterized relative to k reference boxes, which we call anchors. An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure 3, left). By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a convolutional feature map of a size W × H (typically ∼2,400), there are W H k anchors in total.</p>

<p><img src="media/15039097733460/15039114235241.jpg" alt="" title="Region Proposal Network (RPN). Right: Example detections using RPN proposals on PASCAL VOC 2007 test. Our method detects objects in a wide range of scales and aspect ratios."/></p>

<p><strong>Translation-Invariant Anchors</strong></p>

<p>An important property of our approach is that it is translation invariant, both in terms of the anchors and the functions that compute proposals relative to the anchors. If one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either lo- cation. This translation-invariant property is guaran- teed by our method5. As a comparison, the MultiBox method [27] uses k-means to generate 800 anchors, which are not translation invariant. So MultiBox does not guarantee that the same proposal is generated if an object is translated.</p>

<p>The translation-invariant property also reduces the model size. MultiBox has a (4 + 1) × 800-dimensional fully-connected output layer, whereas our method has a (4 + 2) × 9-dimensional convolutional output layer in the case of k = 9 anchors. As a result, our output layer has 2.8 × 104 parameters (512 × (4 + 2) × 9 for VGG-16), two orders of magnitude fewer than MultiBox’s output layer that has 6.1 × 106 parameters (1536 × (4 + 1) × 800 for GoogleNet [34] in MultiBox [27]). If considering the feature projection layers, our proposal layers still have an order of magnitude fewer parameters than MultiBox6. We expect our method to have less risk of overfitting on small datasets, like PASCAL VOC.</p>

<p><strong>Multi-Scale Anchors as Regression References</strong></p>

<p>Our design of anchors presents a novel scheme for addressing multiple scales (and aspect ratios). As shown in Figure 1, there have been two popular ways for multi-scale predictions. The first way is based on image/feature pyramids, e.g., in DPM [8] and CNN- based methods [9], [1], [2]. The images are resized at multiple scales, and feature maps (HOG [8] or deep convolutional features [9], [1], [2]) are computed for each scale (Figure 1(a)). This way is often useful but is time-consuming. The second way is to use sliding windows of multiple scales (and/or aspect ratios) on the feature maps. For example, in DPM [8], models of different aspect ratios are trained separately using different filter sizes (such as 5×7 and 7×5). If this way is used to address multiple scales, it can be thought of as a “pyramid of filters” (Figure 1(b)). The second way is usually adopted jointly with the first way [8].<br/>
As a comparison, our anchor-based method is built on a pyramid of anchors, which is more cost-efficient. Our method classifies and regresses bounding boxes with reference to anchor boxes of multiple scales and aspect ratios. It only relies on images and feature maps of a single scale, and uses filters (sliding win- dows on the feature map) of a single size. We show by experiments the effects of this scheme for addressing multiple scales and sizes (Table 8).<br/>
Because of this multi-scale design based on anchors, we can simply use the convolutional features com- puted on a single-scale image, as is also done by the Fast R-CNN detector [2]. The design of multi- scale anchors is a key component for sharing features without extra cost for addressing scales.</p>

<h4 id="toc_5">3.1.2 LossFunction</h4>

<p>For training RPNs, we assign a binary class label (of being an object or not) to each anchor. We as- sign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-over- Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors. Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample. We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective.<br/>
With these definitions, we minimize an objective function following the multi-task loss in Fast R-CNN [2]. Our loss function for an image is defined as:</p>

<p>\[L(\{p_i\},\{t_i\}) = \frac{1}{N_{cls}}\sum_{cls}(p_i,p_i^*)+\lambda\frac{1}{N_{reg}}\sum_{i}p_i^*L_{reg}(t_i,t_i^*)\]</p>

<p>Here, i is the index of an anchor in a mini-batch and pi is the predicted probability of anchor i being an object. The ground-truth label p∗i is 1 if the anchor is positive, and is 0 if the anchor is negative. ti is a vector representing the 4 parameterized coordinates of the predicted bounding box, and t∗i is that of the ground-truth box associated with a positive anchor. The classification loss Lcls is log loss over two classes (object vs. not object). For the regression loss, we use Lreg (ti, t∗i ) = R(ti − t∗i ) where R is the robust loss function (smooth L1) defined in [2]. The term p∗i Lreg means the regression loss is activated only for positive anchors (p∗i = 1) and is disabled otherwise (p∗i = 0). The outputs of the cls and reg layers consist of {pi} and {ti} respectively.<br/>
The two terms are normalized by Ncls and Nreg and weighted by a balancing parameter λ. In our current implementation (as in the released code), the cls term in Eqn.(1) is normalized by the mini-batch size (i.e., Ncls = 256) and the reg term is normalized by the number of anchor locations (i.e., Nreg ∼ 2, 400). By default we set λ = 10, and thus both cls and reg terms are roughly equally weighted. We show by experiments that the results are insensitive to the values of λ in a wide range (Table 9). We also note that the normalization as above is not required and could be simplified.<br/>
For bounding box regression, we adopt the param- eterizations of the 4 coordinates following [5]:<br/>
tx =(x−xa)/wa, ty =(y−ya)/ha,<br/>
tw = log(w/wa), th = log(h/ha), (2)<br/>
t∗x = (x∗ − xa)/wa, t∗y = (y∗ − ya)/ha, t∗w = log(w∗/wa), t∗h = log(h∗/ha),<br/>
where x, y, w, and h denote the box’s center coordi- nates and its width and height. Variables x, xa, and x∗ are for the predicted box, anchor box, and ground- truth box respectively (likewise for y, w, h). This can be thought of as bounding-box regression from an anchor box to a nearby ground-truth box.<br/>
Nevertheless, our method achieves bounding-box regression by a different manner from previous RoI- based (Region of Interest) methods [1], [2]. In [1], [2], bounding-box regression is performed on features pooled from arbitrarily sized RoIs, and the regression weights are shared by all region sizes. In our formula- tion, the features used for regression are of the same spatial size (3 × 3) on the feature maps. To account for varying sizes, a set of k bounding-box regressors are learned. Each regressor is responsible for one scale and one aspect ratio, and the k regressors do not share weights. As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale, thanks to the design of anchors.</p>

<h4 id="toc_6">3.1.3 Training RPNs</h4>

<p>The RPN can be trained end-to-end by back- propagation and stochastic gradient descent (SGD) [35]. We follow the “image-centric” sampling strategy from [2] to train this network. Each mini-batch arises from a single image that contains many positive and negative example anchors. It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate. Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the mini-batch with negative ones.<br/>
We randomly initialize all new layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0.01. All other layers (i.e., the shared convolutional layers) are initialized by pre- training a model for ImageNet classification [36], as is standard practice [5]. We tune all layers of the ZF net, and conv3 1 and up for the VGG net to conserve memory [2]. We use a learning rate of 0.001 for 60k mini-batches, and 0.0001 for the next 20k mini-batches on the PASCAL VOC dataset. We use a momentum of 0.9 and a weight decay of 0.0005 [37]. Our implementation uses Caffe [38].</p>

<h4 id="toc_7">3.2 Sharing Features for RPN and Fast R-CNN</h4>

<p>Thus far we have described how to train a network for region proposal generation, without considering the region-based object detection CNN that will utilize these proposals. For the detection network, we adopt Fast R-CNN [2]. Next we describe algorithms that learn a unified network composed of RPN and Fast R-CNN with shared convolutional layers (Figure 2).<br/>
Both RPN and Fast R-CNN, trained independently, will modify their convolutional layers in different ways. We therefore need to develop a technique that allows for sharing convolutional layers between the two networks, rather than learning two separate net- works. We discuss three ways for training networks with features shared:</p>

<table>
<thead>
<tr>
<th>Anchor</th>
<th>\(128^2\), 2:1</th>
<th>\(128^2\), 1:1</th>
<th>\(128^2\), 1:2</th>
<th>\(512^2\), 2:1</th>
<th>\(512^2\), 1:1</th>
<th>\(256^2\), 1:2</th>
<th>\(512^2\), 2:1</th>
<th>\(512^2\), 1:1</th>
<th>\(512^2\), 1:2</th>
</tr>
</thead>

<tbody>
<tr>
<td>Proposal</td>
<td>188×111</td>
<td>113×114</td>
<td>70×92</td>
<td>416×229</td>
<td>261×284</td>
<td>174×332</td>
<td>768×437</td>
<td>499×501</td>
<td>355×715</td>
</tr>
</tbody>
</table>

<p>(i) Alternating training. In this solution, we first train RPN, and use the proposals to train Fast R-CNN. The network tuned by Fast R-CNN is then used to initialize RPN, and this process is iterated. This is the solution that is used in all experiments in this paper.</p>

<p>(ii) Approximate joint training. In this solution, the RPN and Fast R-CNN networks are merged into one network during training as in Figure 2. In each SGD iteration, the forward pass generates region propos- als which are treated just like fixed, pre-computed proposals when training a Fast R-CNN detector. The backward propagation takes place as usual, where for the shared layers the backward propagated signals from both the RPN loss and the Fast R-CNN loss are combined. This solution is easy to implement. But this solution ignores the derivative w.r.t. the proposal boxes’ coordinates that are also network responses, so is approximate. In our experiments, we have em- pirically found this solver produces close results, yet reduces the training time by about 25-50% comparing with alternating training. This solver is included in our released Python code.</p>

<p>(iii) Non-approximate joint training. As discussed above, the bounding boxes predicted by RPN are also functions of the input. The RoI pooling layer [2] in Fast R-CNN accepts the convolutional features and also the predicted bounding boxes as input, so a theoretically valid backpropagation solver should also involve gradients w.r.t. the box coordinates. These gradients are ignored in the above approximate joint training. In a non-approximate joint training solution, we need an RoI pooling layer that is differentiable w.r.t. the box coordinates. This is a nontrivial problem and a solution can be given by an “RoI warping” layer as developed in [15], which is beyond the scope of this paper.</p>

<p><strong>4-Step Alternating Training</strong> . In this paper, we adopt a pragmatic 4-step training algorithm to learn shared features via alternating optimization. In the first step, we train the RPN as described in Section 3.1.3. This network is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region pro- posal task. In the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. This detection net- work is also initialized by the ImageNet-pre-trained model. At this point the two networks do not share convolutional layers. In the third step, we use the detector network to initialize RPN training, but we fix the shared convolutional layers and only fine-tune the layers unique to RPN. Now the two networks share convolutional layers. Finally, keeping the shared convolutional layers fixed, we fine-tune the unique layers of Fast R-CNN. As such, both networks share the same convolutional layers and form a unified network. A similar alternating training can be run for more iterations, but we have observed negligible improvements.</p>

<h3 id="toc_8">3.3 Implementation Details</h3>

<p>We train and test both region proposal and object detection networks on images of a single scale [1], [2]. We re-scale the images such that their shorter side is s = 600 pixels [2]. Multi-scale feature extraction (using an image pyramid) may improve accuracy but does not exhibit a good speed-accuracy trade-off [2]. On the re-scaled images, the total stride for both ZF and VGG nets on the last convolutional layer is 16 pixels, and thus is ∼10 pixels on a typical PASCAL image before resizing (∼500×375). Even such a large stride provides good results, though accuracy may be further improved with a smaller stride.<br/>
For anchors, we use 3 scales with box areas of 1282, 2562, and 5122 pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1. These hyper-parameters are not carefully cho- sen for a particular dataset, and we provide ablation experiments on their effects in the next section. As dis- cussed, our solution does not need an image pyramid or filter pyramid to predict regions of multiple scales, saving considerable running time. Figure 3 (right) shows the capability of our method for a wide range of scales and aspect ratios. Table 1 shows the learned average proposal size for each anchor using the ZF net. We note that our algorithm allows predictions that are larger than the underlying receptive field. Such predictions are not impossible—one may still roughly infer the extent of an object if only the middle of the object is visible.<br/>
The anchor boxes that cross image boundaries need to be handled with care. During training, we ignore all cross-boundary anchors so they do not contribute to the loss. For a typical 1000 × 600 image, there will be roughly 20000 (≈ 60 × 40 × 9) anchors in total. With the cross-boundary anchors ignored, there are about 6000 anchors per image for training. If the boundary-crossing outliers are not ignored in training, they introduce large, difficult to correct error terms in the objective, and training does not converge. During testing, however, we still apply the fully convolutional RPN to the entire image. This may generate cross- boundary proposal boxes, which we clip to the image boundary.</p>

<p><img src="media/15039097733460/15039113101725.jpg" alt="" title="Table 2: Detection results on PASCAL VOC 2007 test set (trained on VOC 2007 trainval). The detectors are Fast R-CNN with ZF, but using various proposal methods for training and testing."/></p>

<p>Some RPN proposals highly overlap with each other. To reduce redundancy, we adopt non-maximum suppression (NMS) on the proposal regions based on their cls scores. We fix the IoU threshold for NMS at 0.7, which leaves us about 2000 proposal regions per image. As we will show, NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals. After NMS, we use the top-N ranked proposal regions for detection. In the following, we train Fast R-CNN using 2000 RPN pro- posals, but evaluate different numbers of proposals at test-time.</p>

<h2 id="toc_9">4.实验</h2>

<h2 id="toc_10">5.结论</h2>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">8/28/2017 16:42 下午</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='algorithm.html'>算法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_1.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <h1>AI迷思</h1>
                <div class="site-des">雪山大猫的小窝</div>
                <div class="social">







<a target="_blank" class="weibo" href="http://weibo.com/xueshandamao" title="weibo">Weibo</a>


<a target="_blank" class="email" href="mailto:northland89@163.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="note.html"><strong>课程笔记</strong></a>
        
            <a href="point.html"><strong>心得</strong></a>
        
            <a href="course.html"><strong>课程</strong></a>
        
            <a href="doc.html"><strong>文档</strong></a>
        
            <a href="algorithm.html"><strong>算法</strong></a>
        
            <a href="tech.html"><strong>技术栈</strong></a>
        
            <a href="data.html"><strong>data</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15093533296418.html">软件工程笔记</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15089123496348.html">数据结构笔记</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15077052334055.html">数据结构随堂笔记</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15075982914933.html">数学 10_10 微分方程</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15039097733460.html">Faster RCNN</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
